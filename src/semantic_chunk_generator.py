#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ìƒì„±ê¸°
QA ì˜ë¯¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë¨¼ì € ê·¸ë£¹í™”í•˜ê³ , í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ 500ì ê¸°ì¤€ í›„ë¶„í• 
"""

import json
import fitz
import numpy as np
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Tuple
import re
from pathlib import Path

class SemanticChunkGenerator:
    def __init__(self, model_name: str = "sentence-transformers/all-mpnet-base-v2"):
        """ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        # ëª¨ë¸ ë¡œë”©ì„ ì§€ì—°ì‹œí‚´ (í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)
        self.model_name = model_name
        self.model = None
        
        # ì¼ë°˜ì ì¸ ë¬¸ì„œ ì„¹ì…˜ ì¹´í…Œê³ ë¦¬ (í•˜ë“œì½”ë”©ëœ ë„ë©”ì¸ í‚¤ì›Œë“œ ì œê±°)
        self.general_categories = [
            'ì„œë¡ ', 'ë°°ê²½', 'ëª©ì ', 'ë²”ìœ„', 'ì •ì˜', 'ì ˆì°¨', 'ë°©ë²•', 
            'ê²°ê³¼', 'ê²°ë¡ ', 'ì°¸ê³ ', 'ë¶€ë¡', 'ê¸°íƒ€'
        ]
    
    def _load_model(self):
        """ëª¨ë¸ì„ í•„ìš”í•  ë•Œ ë¡œë“œ"""
        if self.model is None:
            print("[INFO] SentenceTransformer ëª¨ë¸ ë¡œë”© ì¤‘...")
            try:
                # CPU ëª¨ë“œë¡œ ëª¨ë¸ ì´ˆê¸°í™” (CUDA í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
                from sentence_transformers import SentenceTransformer
                self.model = SentenceTransformer(self.model_name, device='cpu')
                print("[SUCCESS] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                print(f"[ERROR] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.model = None

    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì™„í™”ëœ ë²„ì „)"""
        if not text:
            return ""
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = text.strip()
        
        # 2. ì—°ì†ëœ ê°œí–‰ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\n{2,}', '\n', text)
        
        # 3. ë¬¸ì¥ ë ì •ë¦¬
        text = re.sub(r'\n+$', '', text)  # ëì˜ ê°œí–‰ ì œê±°
        text = re.sub(r'^\n+', '', text)  # ì‹œì‘ì˜ ê°œí–‰ ì œê±°
        
        # 4. ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r' +', ' ', text)
        
        # 5. íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì œê±° (Å¸ ë“±)
        text = re.sub(r'[^\uAC00-\uD7AF\u1100-\u11FF\u3130-\u318F\w\s\.\,\;\:\!\?\(\)\[\]\{\}\-\+\=\*\/\@\#\$\%\&\*\(\)]', '', text)
        
        # 6. ì¶”ê°€ ì •ë¦¬: ë¶ˆí•„ìš”í•œ ê°œí–‰ ì œê±°
        text = re.sub(r'\n\s*\n', '\n', text)  # ë¹ˆ ì¤„ ì œê±°
        text = re.sub(r'\n\s+', '\n', text)    # ê°œí–‰ í›„ ê³µë°± ì œê±°
        text = re.sub(r'\s+\n', '\n', text)    # ê°œí–‰ ì „ ê³µë°± ì œê±°
        
        # 7. ì²´í¬ë°•ìŠ¤ë‚˜ ë¼ë””ì˜¤ ë²„íŠ¼ì´ ë§ì€ í…ìŠ¤íŠ¸ ì œê±° (ì™„í™”ëœ ê¸°ì¤€)
        checkbox_count = text.count('â–¡') + text.count('â– ') + text.count('â˜') + text.count('â˜‘')
        if checkbox_count > 10:  # 3ê°œ â†’ 10ê°œë¡œ ì™„í™”
            return ""  # í‘œ/í¼ ë‚´ìš©ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ë°˜í™˜
        
        # 8. "ì˜ˆ ì•„ë‹ˆì˜¤" íŒ¨í„´ì´ ë§ì€ í…ìŠ¤íŠ¸ ì œê±° (ì™„í™”ëœ ê¸°ì¤€)
        yes_no_count = text.count('ì˜ˆ') + text.count('ì•„ë‹ˆì˜¤')
        if yes_no_count > 15:  # 5ê°œ â†’ 15ê°œë¡œ ì™„í™”
            return ""  # ì²´í¬ë°•ìŠ¤ í¼ ë‚´ìš©ì€ ì œê±°
        
        # 9. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì •ë¦¬
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line:  # ë¹ˆ ì¤„ ì œì™¸
                cleaned_lines.append(line)
        
        # 10. ìµœì¢… ì¡°í•©
        text = '\n'.join(cleaned_lines)
        
        # 11. ë§ˆì§€ë§‰ ì •ë¦¬
        text = text.strip()
        
        return text

    def extract_text_blocks(self, pdf_path: str) -> List[Dict]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ (í‘œ ì œì™¸)"""
        print(f"[INFO] PDF í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ: {pdf_path}")
        
        doc = fitz.open(pdf_path)
        all_blocks = []
        
        print(f"[INFO] ì´ í˜ì´ì§€ ìˆ˜: {len(doc)}")
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            print(f"[INFO] í˜ì´ì§€ {page_num + 1}/{len(doc)} ì²˜ë¦¬ ì¤‘...")
            
            try:
                # í‘œ ì˜ì—­ ê°ì§€
                print(f"  [DEBUG] í‘œ ì˜ì—­ ê°ì§€ ì‹œì‘...")
                table_areas = self._detect_table_areas(page)
                print(f"  [DEBUG] ê°ì§€ëœ í‘œ ì˜ì—­: {len(table_areas)}ê°œ")
                
                # í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ (í‘œ ì œì™¸)
                print(f"  [DEBUG] í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ ì‹œì‘...")
                text_blocks = page.get_text("dict")["blocks"]
                print(f"  [DEBUG] ì›ë³¸ í…ìŠ¤íŠ¸ ë¸”ë¡: {len(text_blocks)}ê°œ")
                
                filtered_blocks = self._filter_out_table_blocks(text_blocks, table_areas)
                print(f"  [DEBUG] í•„í„°ë§ í›„ ë¸”ë¡: {len(filtered_blocks)}ê°œ")
                
                # ë¸”ë¡ ì •ë³´ ì €ì¥
                page_blocks = 0
                for i, block in enumerate(filtered_blocks):
                    if "lines" in block:
                        block_text = self._extract_text_from_block(block)
                        if block_text.strip():
                            all_blocks.append({
                                'block_id': f"page_{page_num + 1}_block_{i}",
                                'content': block_text.strip(),
                                'page': page_num + 1,
                                'bbox': block['bbox'],
                                'y_position': block['bbox'][1]
                            })
                            page_blocks += 1
                
                print(f"  [INFO] í˜ì´ì§€ {page_num + 1} ì™„ë£Œ: {page_blocks}ê°œ ë¸”ë¡ ì¶”ì¶œ")
                
            except Exception as e:
                print(f"  [ERROR] í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        doc.close()
        print(f"[SUCCESS] í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ ì™„ë£Œ: {len(all_blocks)}ê°œ")
        return all_blocks
    
    def _detect_table_areas(self, page) -> List:
        """í‘œ ì˜ì—­ ê°ì§€ (ê°•í™”ëœ ë²„ì „)"""
        table_areas = []
        try:
            print(f"    [DEBUG] PyMuPDF í‘œ ê°ì§€ ì‹œì‘...")
            # PyMuPDFì˜ í‘œ ê°ì§€ (ë” ì—„ê²©í•œ ì„¤ì •)
            table_finder = page.find_tables(table_settings={"vertical_strategy": "text", "horizontal_strategy": "text"})
            tables = table_finder.tables
            print(f"    [DEBUG] PyMuPDF ê°ì§€ëœ í‘œ: {len(tables)}ê°œ")
            
            for i, table in enumerate(tables):
                try:
                    # í‘œ í¬ê¸°ì™€ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ í‘œì¸ì§€ ê²€ì¦
                    table_content = table.extract()
                    if self._is_real_table(table_content, table.bbox):
                        table_areas.append(table.bbox)
                        print(f"    [DEBUG] ì‹¤ì œ í‘œ ê°ì§€ë¨: {table.bbox}")
                        print(f"    [DEBUG] í‘œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {str(table_content)[:100]}...")
                    else:
                        print(f"    [DEBUG] ê°€ì§œ í‘œ ì œì™¸ë¨: {table.bbox}")
                except Exception as e:
                    print(f"    [WARNING] í‘œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            # ì¶”ê°€ì ì¸ í‘œ ê°ì§€ ë°©ë²• (í…ìŠ¤íŠ¸ ë¸”ë¡ íŒ¨í„´ ê¸°ë°˜)
            print(f"    [DEBUG] í…ìŠ¤íŠ¸ ë¸”ë¡ íŒ¨í„´ ê°ì§€ ì‹œì‘...")
            text_blocks = page.get_text("dict")["blocks"]
            pattern_detected = 0
            for j, block in enumerate(text_blocks):
                try:
                    if "lines" in block:
                        block_text = ""
                        for line in block["lines"]:
                            for span in line["spans"]:
                                block_text += span["text"] + " "
                        
                        # í‘œ íŒ¨í„´ ê°ì§€ (ë” ì—„ê²©í•œ ê¸°ì¤€)
                        if self._is_table_pattern(block_text):
                            table_areas.append(block["bbox"])
                            pattern_detected += 1
                            print(f"    [DEBUG] í‘œ íŒ¨í„´ ê°ì§€ë¨: {block['bbox']}")
                except Exception as e:
                    print(f"    [WARNING] ë¸”ë¡ {j} íŒ¨í„´ ê°ì§€ ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"    [DEBUG] íŒ¨í„´ ê¸°ë°˜ ê°ì§€: {pattern_detected}ê°œ")
                        
        except Exception as e:
            print(f"    [WARNING] í‘œ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        print(f"    [DEBUG] ì´ ê°ì§€ëœ í‘œ ì˜ì—­: {len(table_areas)}ê°œ")
        return table_areas
    
    def _is_real_table(self, table_content, bbox) -> bool:
        """ì‹¤ì œ í‘œì¸ì§€ ê²€ì¦ (ì™„í™”ëœ ê¸°ì¤€)"""
        if not table_content or len(table_content) == 0:
            return False
        
        # í‘œ í¬ê¸° í™•ì¸ (ì™„í™”ëœ ê¸°ì¤€)
        x1, y1, x2, y2 = bbox
        width = x2 - x1
        height = y2 - y1
        
        # ë„ˆë¬´ í° ì˜ì—­ì€ ì œì™¸ (ì™„í™”ëœ ê¸°ì¤€)
        if width > 500 or height > 400:  # 300x200 â†’ 500x400ìœ¼ë¡œ ì™„í™”
            print(f"[DEBUG] ë„ˆë¬´ í° ì˜ì—­ ì œì™¸: {width:.1f} x {height:.1f}")
            return False
        
        # í‘œ ë‚´ìš© ë¶„ì„
        content_str = str(table_content)
        
        # ì²´í¬ë°•ìŠ¤ë‚˜ ë¼ë””ì˜¤ ë²„íŠ¼ì´ ë§ì€ ê²½ìš° (ì™„í™”ëœ ê¸°ì¤€)
        checkbox_count = content_str.count('â–¡') + content_str.count('â– ') + content_str.count('â˜') + content_str.count('â˜‘')
        if checkbox_count > 8:  # 3ê°œ â†’ 8ê°œë¡œ ì™„í™”
            print(f"[DEBUG] ì²´í¬ë°•ìŠ¤ê°€ ë§ì€ í¼ ì œì™¸: {checkbox_count}ê°œ")
            return False
        
        # ì‹¤ì œ í‘œ êµ¬ì¡° í™•ì¸ (í–‰ê³¼ ì—´ì´ ìˆëŠ”ì§€)
        if isinstance(table_content, list) and len(table_content) > 0:
            # ì²« ë²ˆì§¸ í–‰ì˜ ì—´ ê°œìˆ˜ í™•ì¸
            first_row = table_content[0]
            if isinstance(first_row, list) and len(first_row) > 1:
                # ëª¨ë“  í–‰ì´ ë¹„ìŠ·í•œ ì—´ ê°œìˆ˜ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
                col_counts = [len(row) if isinstance(row, list) else 1 for row in table_content]
                if len(set(col_counts)) <= 2:  # ì—´ ê°œìˆ˜ê°€ ì¼ì •í•˜ë©´ í‘œ
                    return True
        
        return False
    
    def _is_table_pattern(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ í‘œ íŒ¨í„´ì¸ì§€ í™•ì¸ (ì™„í™”ëœ ê¸°ì¤€)"""
        # í‘œ íŒ¨í„´ ê°ì§€ ê·œì¹™
        lines = text.strip().split('\n')
        if len(lines) < 2:
            return False
        
        # ìˆ«ìì™€ í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë˜ëŠ” íŒ¨í„´ í™•ì¸
        has_numbers = any(any(c.isdigit() for c in line) for line in lines)
        has_text = any(any(c.isalpha() for c in line) for line in lines)
        
        # íƒ­ì´ë‚˜ ì—¬ëŸ¬ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ íŒ¨í„´ í™•ì¸
        has_tab_separated = any('\t' in line or '  ' in line for line in lines)
        
        # ì§§ì€ ë¼ì¸ë“¤ì´ ë°˜ë³µë˜ëŠ” íŒ¨í„´ (í‘œì˜ íŠ¹ì§•) - ì™„í™”ëœ ê¸°ì¤€
        short_lines = [line for line in lines if len(line.strip()) < 80]  # 50 â†’ 80ìœ¼ë¡œ ì™„í™”
        has_short_lines = len(short_lines) >= len(lines) * 0.8  # 0.7 â†’ 0.8ë¡œ ì™„í™”
        
        # ì²´í¬ë°•ìŠ¤ë‚˜ ë¼ë””ì˜¤ ë²„íŠ¼ì´ ë§ì€ ê²½ìš° ì œì™¸ (ì™„í™”ëœ ê¸°ì¤€)
        checkbox_count = text.count('â–¡') + text.count('â– ') + text.count('â˜') + text.count('â˜‘')
        if checkbox_count > 8:  # 3ê°œ â†’ 8ê°œë¡œ ì™„í™”
            return False
        
        return (has_numbers and has_text and (has_tab_separated or has_short_lines))
    
    def _filter_out_table_blocks(self, text_blocks: List, table_areas: List) -> List:
        """í‘œ ì˜ì—­ì„ ì œì™¸í•œ í…ìŠ¤íŠ¸ ë¸”ë¡ í•„í„°ë§ (ì™„í™”ëœ ë²„ì „)"""
        filtered_blocks = []
        
        for block in text_blocks:
            if "lines" not in block:
                continue
            
            block_bbox = block["bbox"]
            block_text = ""
            for line in block["lines"]:
                for span in line["spans"]:
                    block_text += span["text"] + " "
            
            # í‘œ ì˜ì—­ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ì™„í™”ëœ ê¸°ì¤€)
            is_in_table = False
            for table_bbox in table_areas:
                if self._bbox_overlap(block_bbox, table_bbox, threshold=0.3):  # 10% â†’ 30%ë¡œ ì™„í™”
                    is_in_table = True
                    print(f"[DEBUG] í‘œ ì˜ì—­ ì œì™¸: {block_bbox}")
                    break
            
            # í‘œ íŒ¨í„´ì¸ì§€ ì¶”ê°€ í™•ì¸ (ì™„í™”ëœ ê¸°ì¤€)
            if not is_in_table and self._is_table_pattern(block_text):
                is_in_table = True
                print(f"[DEBUG] í‘œ íŒ¨í„´ ì œì™¸: {block_bbox}")
            
            # ì²´í¬ë°•ìŠ¤ë‚˜ ë¼ë””ì˜¤ ë²„íŠ¼ì´ ë§ì€ ë¸”ë¡ ì œì™¸ (ì™„í™”ëœ ê¸°ì¤€)
            checkbox_count = block_text.count('â–¡') + block_text.count('â– ') + block_text.count('â˜') + block_text.count('â˜‘')
            if checkbox_count > 5:  # 2ê°œ â†’ 5ê°œë¡œ ì™„í™”
                is_in_table = True
                print(f"[DEBUG] ì²´í¬ë°•ìŠ¤ ë¸”ë¡ ì œì™¸: {block_bbox} ({checkbox_count}ê°œ)")
            
            if not is_in_table:
                filtered_blocks.append(block)
        
        print(f"[INFO] í•„í„°ë§ ê²°ê³¼: {len(text_blocks)}ê°œ ë¸”ë¡ ì¤‘ {len(filtered_blocks)}ê°œ ìœ ì§€")
        return filtered_blocks
    
    def _extract_text_from_block(self, block: Dict) -> str:
        """ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬"""
        text = ""
        for line in block["lines"]:
            for span in line["spans"]:
                text += span["text"] + " "
            # ê°œí–‰ ì œê±°í•˜ê³  ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
            text += " "
        
        # ì „ì²˜ë¦¬ ì ìš©
        cleaned_text = self._preprocess_text(text.strip())
        return cleaned_text
    
    def _bbox_overlap(self, bbox1: List, bbox2: List, threshold: float = 0.3) -> bool:
        """ë‘ bboxê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸"""
        x1, y1, x2, y2 = bbox1
        x3, y3, x4, y4 = bbox2
        
        overlap_x = max(0, min(x2, x4) - max(x1, x3))
        overlap_y = max(0, min(y2, y4) - max(y1, y3))
        
        area1 = (x2 - x1) * (y2 - y1)
        area2 = (x4 - x3) * (y4 - y3)
        overlap_area = overlap_x * overlap_y
        
        if area1 == 0 or area2 == 0:
            return False
        
        overlap_ratio = overlap_area / min(area1, area2)
        return overlap_ratio > threshold
    
    def semantic_clustering(self, text_blocks: List[Dict]) -> List[List[Dict]]:
        """ì˜ë¯¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ì„ë² ë”© ê¸°ë°˜)"""
        print("[INFO] ì˜ë¯¸ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        if len(text_blocks) < 2:
            # ë¸”ë¡ì´ 1ê°œ ì´í•˜ë©´ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ë¡œ ì²˜ë¦¬
            return [text_blocks]
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        texts = [block['content'] for block in text_blocks]
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(texts)
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (ë¬¸ì„œ í¬ê¸°ì— ë”°ë¼ ë™ì  ì¡°ì •)
        n_clusters = min(len(text_blocks) // 3, 8)  # ìµœëŒ€ 8ê°œ í´ëŸ¬ìŠ¤í„°
        n_clusters = max(n_clusters, 2)  # ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„°
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë¸”ë¡ ê·¸ë£¹í™”
        clusters = [[] for _ in range(n_clusters)]
        for i, label in enumerate(cluster_labels):
            clusters[label].append(text_blocks[i])
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥
        for i, cluster in enumerate(clusters):
            print(f"[INFO] í´ëŸ¬ìŠ¤í„° {i+1}: {len(cluster)}ê°œ ë¸”ë¡")
        
        return clusters
    
    def content_based_clustering(self, text_blocks: List[Dict]) -> List[List[Dict]]:
        """ë‚´ìš© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ë” ì¼ë°˜ì ì¸ ë°©ì‹)"""
        print("[INFO] ë‚´ìš© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        if len(text_blocks) < 2:
            return [text_blocks]
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        texts = [block['content'] for block in text_blocks]
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(texts)
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        n_clusters = min(len(text_blocks) // 2, 6)  # ë” ì ì€ í´ëŸ¬ìŠ¤í„° ìˆ˜
        n_clusters = max(n_clusters, 2)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë¸”ë¡ ê·¸ë£¹í™”
        clusters = [[] for _ in range(n_clusters)]
        for i, label in enumerate(cluster_labels):
            clusters[label].append(text_blocks[i])
        
        # ë¹ˆ í´ëŸ¬ìŠ¤í„° ì œê±°
        result_clusters = []
        for i, cluster in enumerate(clusters):
            if cluster:
                result_clusters.append(cluster)
                print(f"[INFO] í´ëŸ¬ìŠ¤í„° {i+1}: {len(cluster)}ê°œ ë¸”ë¡")
        
        return result_clusters
    
    def split_cluster_by_length(self, cluster: List[Dict], max_length: int = 500, overlap_ratio: float = 0.5) -> List[Dict]:
        """í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (overlap í¬í•¨)"""
        chunks = []
        current_chunk = ""
        chunk_id = 1
        overlap_text = ""  # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì €ì¥
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ë¸”ë¡ë“¤ì„ Y ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_blocks = sorted(cluster, key=lambda x: x['y_position'])
        
        for i, block in enumerate(sorted_blocks):
            block_text = block['content']
            
            # í˜„ì¬ ì²­í¬ì— ë¸”ë¡ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
            if len(current_chunk + block_text) <= max_length:
                current_chunk += block_text + " "  # ê°œí–‰ ëŒ€ì‹  ê³µë°± ì‚¬ìš©
            else:
                # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                if current_chunk.strip():
                    # ì „ì²˜ë¦¬ ì ìš©
                    final_chunk = self._preprocess_text(current_chunk.strip())
                    chunks.append({
                        'chunk_id': f"semantic_{chunk_id:03d}",
                        'content': final_chunk,
                        'content_length': len(final_chunk),
                        'chunk_type': 'semantic_clustered',
                        'page': sorted_blocks[0]['page'] if sorted_blocks else 0,  # ì²« ë²ˆì§¸ ë¸”ë¡ì˜ í˜ì´ì§€ ë²ˆí˜¸
                        'y_position': sorted_blocks[0]['y_position'] if sorted_blocks else 0,  # ì²« ë²ˆì§¸ ë¸”ë¡ì˜ Y ìœ„ì¹˜
                        'source_blocks': [b['block_id'] for b in sorted_blocks[:i]],
                        'cluster_category': self._identify_content_category(final_chunk),
                        'overlap_ratio': overlap_ratio
                    })
                    chunk_id += 1
                
                # overlap í…ìŠ¤íŠ¸ ìƒì„± (ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„)
                overlap_text = self._create_overlap_text(current_chunk, overlap_ratio)
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (overlap í…ìŠ¤íŠ¸ í¬í•¨)
                current_chunk = overlap_text + block_text + " "  # ê°œí–‰ ëŒ€ì‹  ê³µë°± ì‚¬ìš©
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            # ì „ì²˜ë¦¬ ì ìš©
            final_chunk = self._preprocess_text(current_chunk.strip())
            chunks.append({
                'chunk_id': f"semantic_{chunk_id:03d}",
                'content': final_chunk,
                'content_length': len(final_chunk),
                'chunk_type': 'semantic_clustered',
                'page': sorted_blocks[0]['page'] if sorted_blocks else 0,  # ì²« ë²ˆì§¸ ë¸”ë¡ì˜ í˜ì´ì§€ ë²ˆí˜¸
                'y_position': sorted_blocks[0]['y_position'] if sorted_blocks else 0,  # ì²« ë²ˆì§¸ ë¸”ë¡ì˜ Y ìœ„ì¹˜
                'source_blocks': [b['block_id'] for b in sorted_blocks],
                'cluster_category': self._identify_content_category(final_chunk),
                'overlap_ratio': overlap_ratio
            })
        
        return chunks
    
    def _create_overlap_text(self, text: str, overlap_ratio: float = 0.5) -> str:
        """ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ overlap í…ìŠ¤íŠ¸ë¡œ ìƒì„±"""
        if not text.strip():
            return ""
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(text)
        
        if not sentences:
            return ""
        
        # overlap_ratioì— ë”°ë¼ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ì„ íƒ
        overlap_count = max(1, int(len(sentences) * overlap_ratio))
        overlap_sentences = sentences[-overlap_count:]
        
        # overlap í…ìŠ¤íŠ¸ ìƒì„±
        overlap_text = " ".join(overlap_sentences)
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì „ì²´ í…ìŠ¤íŠ¸ì˜ 50% ì´í•˜)
        max_overlap_length = len(text) * 0.5
        if len(overlap_text) > max_overlap_length:
            # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            words = overlap_text.split()
            overlap_text = " ".join(words[:int(max_overlap_length / 5)])  # í‰ê·  ë‹¨ì–´ ê¸¸ì´ 5ì ê°€ì •
        
        return overlap_text + " "  # ê°œí–‰ ëŒ€ì‹  ê³µë°± ì‚¬ìš©
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„ íŒ¨í„´
        sentence_pattern = r'[^.!?ã€‚]*[.!?ã€‚]'
        sentences = re.findall(sentence_pattern, text)
        
        # íŒ¨í„´ì— ë§¤ì¹˜ë˜ì§€ ì•Šì€ ë¶€ë¶„ë„ ì¶”ê°€
        remaining = re.sub(sentence_pattern, '', text)
        if remaining.strip():
            sentences.append(remaining.strip())
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _identify_content_category(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ë‚´ìš© ì¹´í…Œê³ ë¦¬ ì‹ë³„ (ë” ì¼ë°˜ì ì¸ ë°©ì‹)"""
        text_lower = text.lower()
        
        # ì¼ë°˜ì ì¸ ë¬¸ì„œ ì„¹ì…˜ í‚¤ì›Œë“œë¡œ ì¹´í…Œê³ ë¦¬ ì‹ë³„
        if any(word in text_lower for word in ['ì„œë¡ ', 'ê°œìš”', 'ë°°ê²½', 'ëª©ì ']):
            return 'ì„œë¡ '
        elif any(word in text_lower for word in ['ë°©ë²•', 'ì ˆì°¨', 'ê³¼ì •', 'ë‹¨ê³„']):
            return 'ë°©ë²•'
        elif any(word in text_lower for word in ['ê²°ê³¼', 'ì„±ê³¼', 'íš¨ê³¼', 'ì„±ëŠ¥']):
            return 'ê²°ê³¼'
        elif any(word in text_lower for word in ['ê²°ë¡ ', 'ìš”ì•½', 'ì •ë¦¬']):
            return 'ê²°ë¡ '
        elif any(word in text_lower for word in ['ì°¸ê³ ', 'ë¶€ë¡', 'ì²¨ë¶€']):
            return 'ì°¸ê³ '
        else:
            return 'ë³¸ë¬¸'
    
    def sequential_chunking(self, text_blocks: List[Dict], max_length: int = 500, overlap_ratio: float = 0.5) -> List[Dict]:
        """ìˆœì°¨ì  ì²­í‚¹ (ë¬¸ì„œ ìˆœì„œ ê·¸ëŒ€ë¡œ)"""
        chunks = []
        current_chunk = ""
        chunk_id = 1
        overlap_text = ""
        
        for i, block in enumerate(text_blocks):
            block_text = block['content']
            
            # í˜„ì¬ ì²­í¬ì— ë¸”ë¡ì„ ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
            if len(current_chunk + block_text) <= max_length:
                current_chunk += block_text + " "
            else:
                # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                if current_chunk.strip():
                    final_chunk = self._preprocess_text(current_chunk.strip())
                    
                    chunks.append({
                        'chunk_id': f"sequential_{chunk_id:03d}",
                        'content': final_chunk,
                        'content_length': len(final_chunk),
                        'chunk_type': 'sequential_chunked',
                        'page': text_blocks[i-1]['page'] if i > 0 else 0,
                        'y_position': text_blocks[i-1]['y_position'] if i > 0 else 0,
                        'source_blocks': [b['block_id'] for b in text_blocks[max(0, i-len(current_chunk.split())):i]],
                        'overlap_ratio': overlap_ratio
                    })
                    chunk_id += 1
                
                # overlap í…ìŠ¤íŠ¸ ìƒì„±
                overlap_text = self._create_overlap_text(current_chunk, overlap_ratio)
                current_chunk = overlap_text + block_text + " "
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            final_chunk = self._preprocess_text(current_chunk.strip())
            
            chunks.append({
                'chunk_id': f"sequential_{chunk_id:03d}",
                'content': final_chunk,
                'content_length': len(final_chunk),
                'chunk_type': 'sequential_chunked',
                'page': text_blocks[-1]['page'] if text_blocks else 0,
                'y_position': text_blocks[-1]['y_position'] if text_blocks else 0,
                'source_blocks': [b['block_id'] for b in text_blocks[-len(current_chunk.split()):]],
                'overlap_ratio': overlap_ratio
            })
        
        return chunks
    
    def generate_semantic_chunks(self, pdf_path: str) -> List[Dict]:
        """ìˆœì°¨ì  ì²­í¬ ìƒì„± (ì˜ë¯¸ì  ë³´ì • í¬í•¨)"""
        print("=" * 60)
        print("ìˆœì°¨ì  ì²­í¬ ìƒì„± ì‹œì‘")
        print("=" * 60)
        
        try:
            # 1. í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ
            print("[STEP 1] í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ ì‹œì‘...")
            text_blocks = self.extract_text_blocks(pdf_path)
            
            if not text_blocks:
                print("[WARNING] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            print(f"[STEP 1] ì™„ë£Œ: {len(text_blocks)}ê°œ ë¸”ë¡ ì¶”ì¶œ")
            
            # 2. í˜ì´ì§€ ë° ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
            print("[STEP 2] ë¸”ë¡ ì •ë ¬ ì‹œì‘...")
            sorted_blocks = sorted(text_blocks, key=lambda x: (x['page'], x['y_position']))
            print(f"[STEP 2] ì™„ë£Œ: {len(sorted_blocks)}ê°œ ë¸”ë¡ ì •ë ¬")
            
            # 3. ìˆœì°¨ì  ì²­í‚¹ (ì˜ë¯¸ì  ë³´ì • í¬í•¨)
            print("[STEP 3] ìˆœì°¨ì  ì²­í‚¹ ì‹œì‘...")
            all_chunks = self.sequential_chunking(sorted_blocks, max_length=300, overlap_ratio=0.5)
            print(f"[STEP 3] ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # 4. ê²°ê³¼ ì €ì¥
            print("[STEP 4] ê²°ê³¼ ì €ì¥ ì‹œì‘...")
            output_file = "semantic_chunks.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_chunks, f, ensure_ascii=False, indent=2)
            
            print(f"\n[SUCCESS] ìˆœì°¨ì  ì²­í¬ ìƒì„± ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬")
            print(f"[INFO] ê²°ê³¼ ì €ì¥: {output_file}")
            print(f"[INFO] ì²­í¬ ê¸¸ì´: 300ì (overlap: 50%)")
            
            # ì²­í¬ ê¸¸ì´ í†µê³„
            if all_chunks:
                lengths = [chunk['content_length'] for chunk in all_chunks]
                avg_length = sum(lengths) / len(lengths) if lengths else 0
                print(f"\nğŸ“Š ì²­í¬ í†µê³„:")
                print(f"  - í‰ê·  ê¸¸ì´: {avg_length:.1f}ì")
                print(f"  - ìµœì†Œ ê¸¸ì´: {min(lengths)}ì")
                print(f"  - ìµœëŒ€ ê¸¸ì´: {max(lengths)}ì")
            
            return all_chunks
            
        except Exception as e:
            print(f"[ERROR] ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []

def main():
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python semantic_chunk_generator.py <PDFíŒŒì¼ê²½ë¡œ>")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    
    if not Path(pdf_path).exists():
        print(f"[ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        sys.exit(1)
    
    # ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ìƒì„±ê¸° ì‹¤í–‰
    generator = SemanticChunkGenerator()
    generator.generate_semantic_chunks(pdf_path)

if __name__ == "__main__":
    main() 